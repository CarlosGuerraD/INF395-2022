{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f98714ff-5899-4f05-935f-e6f70ec12251",
   "metadata": {},
   "source": [
    "## LibrerÃ­as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "79a3eb12-b816-4969-a48f-89e9addc9366",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import librosa\n",
    "import librosa.display\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import matplotlib as plt\n",
    "import sklearn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d5f8749a-cacc-46b4-830c-216db16c2260",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15131711-5e42-4964-80a0-8b569cc2a4d8",
   "metadata": {},
   "source": [
    "## Lectura de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaebf4c8-4aa4-4d1e-9e54-fd526938c208",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_fpath = \"../desafio/data/Train/\"\n",
    "audio_clips = os.listdir(audio_fpath)\n",
    "print(\"No. of .wav files in audio folder = \",len(audio_clips))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d85aac-490a-40b4-83fb-bb7285c0d30f",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_clips[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "179db9dc-447d-425a-9afd-6d29629459fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "x , sr = librosa.load(f\"../desafio/data/Train/{audio_clips[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "628c70b4-9f17-44db-9f7c-7bbf41abb033",
   "metadata": {},
   "outputs": [],
   "source": [
    "x.shape,sr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c5eea0-7136-4c3d-ba5a-c2fb7496fe97",
   "metadata": {},
   "outputs": [],
   "source": [
    "librosa.display.waveshow(x, sr=sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d601648-5103-4502-9114-b5d6da1fe031",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = librosa.stft(x)\n",
    "Xdb = librosa.amplitude_to_db(abs(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6141e6b1-6e1e-4cf3-9806-9264e9f2c778",
   "metadata": {},
   "outputs": [],
   "source": [
    "librosa.display.specshow(Xdb,sr=sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4560ca6a-8dfb-4dd5-a471-bf3c14c7f408",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xdb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9010a6f1-cc93-47e3-956d-6efc3424959c",
   "metadata": {},
   "source": [
    "## Procesamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5897f742-80e4-428f-9f11-9282faa1fcd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_test = []\n",
    "#for clip in audio_clips:\n",
    "#    x , sr = librosa.load(f\"../desafio/data/Train/{clip}\")\n",
    "#    X = librosa.stft(x)\n",
    "#    Xdb = librosa.amplitude_to_db(abs(X))\n",
    "#    X_test.append(Xdb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6febce5f-8d69-473e-a9f5-a85d23fc40ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#file = open(\"data.obj\", \"wb\")\n",
    "#data = pickle.dump(X_test, file)\n",
    "#file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "287898cb-5a7b-4262-8ad0-6ab4959e78d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"data.obj\", \"rb\")\n",
    "data = pickle.load(file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c6a70c-81dd-4216-bfb0-f7d23874f887",
   "metadata": {},
   "outputs": [],
   "source": [
    "maxshape = 0\n",
    "for s in data:\n",
    "    if s.shape[1] >= maxshape:\n",
    "        maxshape = s.shape[1]\n",
    "data[0].shape[0],maxshape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb344638-0dba-4f2d-9c92-5d774736bc62",
   "metadata": {},
   "outputs": [],
   "source": [
    "datareshape = []\n",
    "for item in data:\n",
    "    i,j = item.shape\n",
    "    fill_left = np.floor((maxshape-j)/2)\n",
    "    fill_right = np.ceil((maxshape-j)/2)\n",
    "    datareshape.append(np.pad(item, ((0,0),(int(fill_left),int(fill_right))), mode='constant', constant_values=np.min(item)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd0bbbba-bcc8-4874-a600-58529353045a",
   "metadata": {},
   "outputs": [],
   "source": [
    "librosa.display.specshow(datareshape[0],sr=sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a0fca7e-bdda-49ad-aac7-52be87e1d239",
   "metadata": {},
   "outputs": [],
   "source": [
    "npdata = np.asarray(datareshape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6bddb0f-cc7f-4c00-89f6-37a8cf3fa7f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "del data,datareshape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f6813d-7100-4b46-8693-1b0a881a1171",
   "metadata": {},
   "outputs": [],
   "source": [
    "#file = open(\"npdata.obj\", \"wb\")\n",
    "#data = pickle.dump(npdata, file)\n",
    "#file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0801e7d-3a95-492e-8c47-d27a28480c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"npdata.obj\", \"rb\")\n",
    "npdata = pickle.load(file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b06cca32-dfc9-496b-801b-b485adac2d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "npdata.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b19af8-3e76-4916-a8e4-06ddff52c407",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_tf = tf.convert_to_tensor(npdata, np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "894348d9-0f7f-48aa-b89f-ff5d599bba52",
   "metadata": {},
   "source": [
    "## Data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a24d927-30fd-4562-b466-c5f2b4e5983f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nlpaug.flow as naf\n",
    "import nlpaug.augmenter.spectrogram as nas\n",
    "from nlpaug.util.audio.visualizer import AudioVisualizer\n",
    "\n",
    "\n",
    "test = npdata[1]\n",
    "flow = naf.Sequential([\n",
    "    nas.FrequencyMaskingAug(zone=(0,1)), \n",
    "    nas.LoudnessAug(coverage=1,factor=(0.8,1.5))\n",
    "])\n",
    "aug_test = flow.augment(test)\n",
    "\n",
    "\n",
    "librosa.display.specshow(test,sr=sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce7ed74-c541-4209-bc4f-5f7cec884b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "reshape_test = np.reshape(np.asarray(aug_test), (1025, 537))\n",
    "librosa.display.specshow(reshape_test,sr=sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f835df0-8592-4535-beeb-9a127032484c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment(image):\n",
    "    flow = naf.Sequential([\n",
    "        nas.FrequencyMaskingAug(zone=(0,1)), \n",
    "        nas.FrequencyMaskingAug(zone=(0,1)), \n",
    "        nas.TimeMaskingAug(coverage=.00001), \n",
    "        nas.LoudnessAug(coverage=.0001)\n",
    "    ])\n",
    "    temp_image = flow.augment(image)\n",
    "    aug_image = np.reshape(np.asarray(temp_image), image.shape)\n",
    "    return aug_image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "326f9177-3273-4047-82f0-2a2f0c716fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from IPython.display import clear_output\n",
    "#import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "604d647e-bda5-4fed-976b-0bfe7b9b498b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for i in range(500):\n",
    "#    librosa.display.specshow(augment_data[i],sr=sr)\n",
    "#    plt.pyplot.show()\n",
    "#    clear_output(wait = True)\n",
    "#    time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "628b0c1a-702f-4958-bcd0-a0d734c628aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3443cf8c-aa8d-4130-a586-8a164f78116b",
   "metadata": {},
   "outputs": [],
   "source": [
    "extended_data = npdata\n",
    "for i in range(5):\n",
    "    augment_data = np.array(list(map(augment, npdata)))\n",
    "    extended_data = np.append(extended_data, augment_data, axis=0)\n",
    "del augment_data,npdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f071065e-daaf-4150-b52d-12f99696ade7",
   "metadata": {},
   "outputs": [],
   "source": [
    "extended_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec57bbb-f73e-4f55-88e5-c5159a88f1e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#file = open(\"extended_data.obj\", \"wb\")\n",
    "#pickle.dump(extended_data, file)\n",
    "#file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ff89097-e92f-458d-9cfc-17b33895d22a",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"extended_data.obj\", \"rb\")\n",
    "extended_data = pickle.load(file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20085510-7219-43fb-b4e5-8bfd355ebb2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d442a3-77d0-4286-b90b-aea734bce987",
   "metadata": {},
   "outputs": [],
   "source": [
    "s.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f16c428-0107-404a-a107-b1cbfe4068aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(s, axis = 0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2096feb-86c4-44db-acf0-eaa3f45f3684",
   "metadata": {},
   "outputs": [],
   "source": [
    "extended_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f391a79d-de61-42b6-9992-d9c7cf6be047",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = np.mean(extended_data,axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13696bf9-4739-4cef-8da0-6c3aeb2b6775",
   "metadata": {},
   "outputs": [],
   "source": [
    "std = np.std(extended_data,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cacfa986-4330-423d-9bd8-ef3daee4fae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#file = open(\"std.obj\", \"wb\")\n",
    "#pickle.dump(std, file)\n",
    "#file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ecd2af9-9fcf-4e34-802d-d2c5cb321d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"mean.obj\", \"rb\")\n",
    "mean = pickle.load(file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "62a86041-3137-40b5-a299-ef75db868842",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"std.obj\", \"rb\")\n",
    "std = pickle.load(file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd98835f-aac0-4d6f-ab87-cc139dc8cd84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# file = open(\"mean.obj\", \"wb\")\n",
    "# pickle.dump(mean, file)\n",
    "# file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "894ca32c-41eb-46a1-875a-71f6863fa7f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "extended_data=extended_data/std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "35716f91-5286-4274-a5c9-fdec1688510e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#file = open(\"normalized.obj\", \"wb\")\n",
    "#pickle.dump(extended_data, file)\n",
    "#file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7616421c-1dca-48b8-bbeb-62caa54b543b",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"normalized.obj\", \"rb\")\n",
    "normalized = pickle.load(file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f088d53a-deb5-4d06-827f-da6d7cd6cb1a",
   "metadata": {},
   "source": [
    "## Lectura de vector objetivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "89e7be2d-c7e5-4d1f-aaf7-5dfb3f6b87a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_genre = pd.read_csv('../desafio/data/Train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8b842bf8-cdbe-4bf3-ac34-70b16190c07a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Expected</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>01216683570.wav</td>\n",
       "      <td>0 2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00433588573.wav</td>\n",
       "      <td>0 2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00381534896.wav</td>\n",
       "      <td>0 2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>01635825413.wav</td>\n",
       "      <td>0 2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00325117692.wav</td>\n",
       "      <td>0 2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Id Expected\n",
       "0  01216683570.wav      0 2\n",
       "1  00433588573.wav      0 2\n",
       "2  00381534896.wav      0 2\n",
       "3  01635825413.wav      0 2\n",
       "4  00325117692.wav      0 2"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_genre.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "20200afb-ec83-40b1-afa4-5d5207937426",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_genre['Expected'] = df_genre['Expected'].apply(lambda x: x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "16519fc7-2aaa-470e-9e81-e1a4d3443e0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Expected</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>01216683570.wav</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00433588573.wav</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00381534896.wav</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>01635825413.wav</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00325117692.wav</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Id Expected\n",
       "0  01216683570.wav        0\n",
       "1  00433588573.wav        0\n",
       "2  00381534896.wav        0\n",
       "3  01635825413.wav        0\n",
       "4  00325117692.wav        0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_genre.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0d5d0cad-92c3-4ecb-b360-99d767e5e4b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. ... 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "y1 = np.array(list(map(float, df_genre[\"Expected\"])))\n",
    "print(y1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f668e14e-5acc-4d5f-957c-ca587e20b373",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8370,)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extended_y = y1\n",
    "for i in range(5):\n",
    "    extended_y=np.append(extended_y, y1, axis=0)\n",
    "extended_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "93e67312-4271-4c77-8246-77044fda7f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.model_selection import train_test_split\n",
    "#\n",
    "#X1_tr, X1_val, y1_tr, y1_val = train_test_split(\n",
    "#    extended_data, extended_y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b3346ba0-608e-4c59-ba39-6283f518a338",
   "metadata": {},
   "outputs": [],
   "source": [
    "#del extended_data,extended_y,y1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36571c73-98d5-43c0-9831-3f43c228a99c",
   "metadata": {},
   "source": [
    "## Red Masculino/Femenino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "cd685f37-5518-4427-85be-054051d14613",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Conv2D(32, 3, strides=2, activation='relu', input_shape = [1025,537,1]),\n",
    "    keras.layers.MaxPooling2D(pool_size=(3, 3)),\n",
    "    keras.layers.Conv2D(64, 2, strides=2, activation='relu'),\n",
    "    keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "    keras.layers.Conv2D(128, 2, strides=2, activation='relu'),\n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dense(50, activation='relu'),\n",
    "    keras.layers.Dense(1, activation ='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "756b11a5-53b5-49a2-ac19-8e8415189c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=keras.optimizers.Adam(0.001),\n",
    "             loss = 'binary_crossentropy',\n",
    "             metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ca10dbf6-b7e9-4689-9554-1e1f48a16781",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_15 (Conv2D)          (None, 512, 268, 32)      320       \n",
      "                                                                 \n",
      " max_pooling2d_11 (MaxPoolin  (None, 170, 89, 32)      0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_16 (Conv2D)          (None, 85, 44, 64)        8256      \n",
      "                                                                 \n",
      " max_pooling2d_12 (MaxPoolin  (None, 42, 22, 64)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_17 (Conv2D)          (None, 21, 11, 128)       32896     \n",
      "                                                                 \n",
      " flatten_4 (Flatten)         (None, 29568)             0         \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 50)                1478450   \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 1)                 51        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,519,973\n",
      "Trainable params: 1,519,973\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9d4dae67-369d-46b5-891f-f8b7a8acf616",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.preprocessing import StandardScaler\n",
    "#\n",
    "#scaler_x1 = StandardScaler()\n",
    "#scaler_x1.fit(X1)\n",
    "#x1_tr = scaler_x1.transform(X1_tr)\n",
    "#x1_val = scaler_x1.transform(X1_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7bdac926-ec0d-4e03-ac5e-4f76543c6651",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 50\n",
    "callback = keras.callbacks.EarlyStopping(monitor='val_loss',\n",
    "                                            patience=5,\n",
    "                                            restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a1d1bdb0-8296-4087-be18-d16522b5c362",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "236/236 [==============================] - 174s 735ms/step - loss: 0.6928 - accuracy: 0.5364 - val_loss: 0.7110 - val_accuracy: 0.3214\n",
      "Epoch 2/50\n",
      "236/236 [==============================] - 165s 700ms/step - loss: 0.6806 - accuracy: 0.5615 - val_loss: 0.7034 - val_accuracy: 0.5161\n",
      "Epoch 3/50\n",
      "236/236 [==============================] - 165s 698ms/step - loss: 0.6374 - accuracy: 0.6320 - val_loss: 0.7257 - val_accuracy: 0.5412\n",
      "Epoch 4/50\n",
      "236/236 [==============================] - 161s 683ms/step - loss: 0.5149 - accuracy: 0.7387 - val_loss: 0.5410 - val_accuracy: 0.7121\n",
      "Epoch 5/50\n",
      "236/236 [==============================] - 162s 688ms/step - loss: 0.3273 - accuracy: 0.8505 - val_loss: 0.3672 - val_accuracy: 0.8351\n",
      "Epoch 6/50\n",
      "236/236 [==============================] - 163s 692ms/step - loss: 0.1671 - accuracy: 0.9323 - val_loss: 0.3556 - val_accuracy: 0.8542\n",
      "Epoch 7/50\n",
      "236/236 [==============================] - 163s 689ms/step - loss: 0.0980 - accuracy: 0.9643 - val_loss: 0.3590 - val_accuracy: 0.8566\n",
      "Epoch 8/50\n",
      "236/236 [==============================] - 165s 699ms/step - loss: 0.0559 - accuracy: 0.9800 - val_loss: 0.6362 - val_accuracy: 0.7981\n",
      "Epoch 9/50\n",
      "236/236 [==============================] - 162s 686ms/step - loss: 0.0326 - accuracy: 0.9899 - val_loss: 0.5039 - val_accuracy: 0.8495\n",
      "Epoch 10/50\n",
      "236/236 [==============================] - 161s 682ms/step - loss: 0.0382 - accuracy: 0.9891 - val_loss: 0.4545 - val_accuracy: 0.8435\n",
      "Epoch 11/50\n",
      "236/236 [==============================] - 164s 693ms/step - loss: 0.0213 - accuracy: 0.9940 - val_loss: 0.5463 - val_accuracy: 0.8530\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(extended_data,extended_y, epochs=epochs, validation_split=0.1,callbacks=callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2bf8cec8-f87e-47f0-bfc2-c5df6f4b786c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 3 of 3). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Redes/desafio/models/Gender2w/normalization\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Redes/desafio/models/Gender2w/normalization\\assets\n"
     ]
    }
   ],
   "source": [
    "model.save(\"../Redes/desafio/models/Gender2w/normalization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a5f3f7d1-76a2-4145-b9fb-76e5f9f46ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights(\"../Redes/desafio/models/Gender2_weights\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "910f3ad2-7d71-4f69-b7ef-3d012fbb87bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.models.load_model(\"../Redes/desafio/models/Gender2w/normalization\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
